# Kaggle Data Science Competitions

### 1. Titanic: Machine Learning from Disaster

Link: https://www.kaggle.com/c/titanic

In this challenge, I used many machine learning tools to predict which passengers survived the Titanic tragedy. 

In particular, the training and testing data are feature scaled based on "name", "ticket class", "sex", "age", "number of siblings and spouses aboard the Titanic", "number of parents and children aboard the Titanic", "ticket number", "passenger fare", "cabin number", and "port of embarkation".

For model selection, I used Logistic Regression, Random Forest Classification, and Gradient Boosting Classification.

The best result is based on Logistic Regression, which is 79.904% accurracy and ranked 1699 out of 9605 teams in this Kaggle Competition.


### 2. Home Credit Default Risk

Link: https://www.kaggle.com/c/home-credit-default-risk

In this challenge, we used the credit data from Home Credit to predict if a client is able to repay the loan. 

The dataset contains around 300,000 instances with more than 100 features, including data from credit bureau, previous application, current application, monthly balances, installments payments, etc. 

The model we used is LightGBM, ensembled with other models in public kernels. Currently, we achieved an AUC of 0.800, which is ranked top 8.8% on the public leaderboard. 



